# -*- coding: utf-8 -*-
"""ANN_A-BCD_sim.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yDvO88xzE7osY4XgkJkWrpz_vgB9gmwC
"""

from google.colab import drive
drive.mount('/content/drive')

"""#Funciones"""

!pip install git+https://github.com/tensorflow/docs
!pip install pytictoc

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
import pandas as pd
from scipy.interpolate import griddata
from sklearn.utils import shuffle
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import r2_score
from numpy import mean, std
from sklearn.metrics import mean_squared_error
from IPython.display import Image
import pickle
from sklearn.linear_model import LinearRegression

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD, RMSprop, Adam
import tensorflow_docs as tfdocs
import tensorflow_docs.modeling
from pytictoc import TicToc

def getIndexes(dfObj, value):
    ''' Get index positions of value in dataframe i.e. dfObj.'''
    ''' dfObj: pandas DataFrame'''
    ''' value: value that wants to be searched in the DataFrame '''
    listOfPos = []
    # Get bool dataframe with True at positions where the given value exists
    result = dfObj.isin([value])
    # Get list of columns that contains the value
    seriesObj = result.any()
    columnNames = (seriesObj[seriesObj == True].index)
    # Iterate over list of columns and fetch the rows indexes where value exists
    for col in columnNames:
        rows = (result[col][result[col] == True].index)
        for row in rows:
            listOfPos.append((row))
    # Return a list of tuples indicating the positions of value in the dataframe
    return listOfPos

def filter(Data,Filter,Optimize):
  ''' Filter a DataFrame just keeping the optimized sample point'''
  ''' DataFrame: the matrix of DataPoints (pd.DataFrame)'''
  ''' Filter: name of the column that wants to be filtered (string)'''
  ''' Optimize: name of the reference column that will be optimized (string)'''
  dt = Data
  dt = dt.sort_values(Filter,axis=0, ascending=False)
  dt.index = range(dt.shape[0])        
  ini = -1
  index = 0
  res = pd.DataFrame()
  for i in dt[Filter]:
    if ini != i:
      ini = i
      best_ind = getIndexes(dt,i)
      #best_ind és un vector que conté els vectors amb les posicions dels punts amb mateix valor en la matriu original (enumerats del 0 al 9999).
      best = []
      for j in best_ind:
        best.append(dt.iloc[j,:])
      #Fins aquí està tot bé. Cada best té la matriu corresponent als punts best_ind anteriors.
      best = pd.DataFrame(best)
      res_ind = getIndexes(best,best[Optimize].min())
      #Fins aquí tot bé. S'escull la posició adequada de la matriu best. La posició té com a índex el valor entre 0-9999 que li correspon del dt original, no el índex en la matriu best (està nice).
      res = pd.concat([res, dt.iloc[res_ind,:]], axis=0,)
  res = pd.DataFrame(res)
  res = res.sort_values(Filter,axis=0, ascending=False)
  return res

def HeatMap_nrm(Data,var1,var2,var3,nrm_mean,nrm_std):
  ''' Convert from pandas dataframes to numpy arrays '''
  ''' Data: pandas DataFrame '''
  ''' var1,var2: columns from the DataFrame that want to be plot '''
  dt = Data
  X, Y, Z, = np.array([]), np.array([]), np.array([])
  for i in range(len(dt.iloc[:,0])):
          X = np.append(X, dt.iloc[i,var1] * nrm_std[var1] + nrm_mean[var1])
          Y = np.append(Y, dt.iloc[i,var2] * nrm_std[var2] + nrm_mean[var2])
          Z = np.append(Z, dt.iloc[i,var3] * nrm_std[var3] + nrm_mean[var3])

  # create x-y points to be used in heatmap
  xi = np.linspace(X.min(), X.max(), int(len(dt.iloc[:,0])**0.5))
  yi = np.linspace(Y.min(), Y.max(), int(len(dt.iloc[:,0])**0.5))

  # Interpolate for plotting
  zi = griddata((X, Y), Z, (xi[None,:], yi[:,None]), method='cubic')

  # I control the range of my colorbar by removing data 
  # outside of my range of interest
  zmin = dt.iloc[:,var3].min()* nrm_std[var3] + nrm_mean[var3]
  zmax = dt.iloc[:,var3].max()* nrm_std[var3] + nrm_mean[var3]
  zi[(zi<zmin) | (zi>zmax)] = None

  # Create the contour plot
  CS = plt.contourf(xi, yi, zi, 30, cmap=plt.cm.rainbow,
                    vmax=zmax, vmin=zmin)
  CS.cmap.set_over('red')
  CS.cmap.set_under('blue')
  CS.changed()
  plt.colorbar()
  """
  names = list(dt)
  CS.set_xlabel(names[var1], fontsize=14)
  CS.set_ylabel(names[var2], fontsize=14)
  CS.set_title('Heat Map 2D normalized', fontsize=20)
  cbar.CS.get_yaxis().labelpad = 10
  cbar.CS.set_ylabel(names[var3], fontsize=14)
  """
  return plt.show()

def HeatMap(Data,var1,var2,var3):
  ''' Convert from pandas dataframes to numpy arrays '''
  ''' Data: pandas DataFrame '''
  ''' var1,var2, var3: columns from the DataFrame that want to be plot '''
  dt = Data
  X, Y, Z, = np.array([]), np.array([]), np.array([])
  for i in range(len(dt.iloc[:,0])):
          X = np.append(X, dt.iloc[i,var1])
          Y = np.append(Y, dt.iloc[i,var2])
          Z = np.append(Z, dt.iloc[i,var3])

  # create x-y points to be used in heatmap
  xi = np.linspace(X.min(), X.max(), 1000)
  yi = np.linspace(Y.min(), Y.max(), 1000)

  # Interpolate for plotting
  zi = griddata((X, Y), Z, (xi[None,:], yi[:,None]), method='cubic')

  # I control the range of my colorbar by removing data 
  # outside of my range of interest
  zmin = dt.iloc[:,var3].min()
  zmax = dt.iloc[:,var3].max()
  zi[(zi<zmin) | (zi>zmax)] = None

  # Create the contour plot
  CS = plt.contourf(xi, yi, zi, 30, cmap=plt.cm.rainbow,
                    vmax=zmax, vmin=zmin)
  CS.cmap.set_over('red')
  CS.cmap.set_under('blue')
  CS.changed()
  plt.colorbar()
  """
  names = list(dt)
  CS.xlabel(names[var1], fontsize=14)
  CS.ylabel(names[var2], fontsize=14)
  CS.title('Heat Map 2D', fontsize=20)
  cbar.CS.get_yaxis().labelpad = 10
  cbar.CS.set_ylabel(names[var3], fontsize=14)
  """
  return plt.show()

def grid_3D(low,up,points):
  ''' Creates a 3D grid that can be used to draw the model'''
  ''' low, up, points: integrers'''
  x1 = np.linspace(low[0],up[0],points)
  x2 = np.linspace(low[1],up[1],points)
  x3 = np.linspace(low[2],up[2],points)
  
  x1, x2, x3 = np.meshgrid(x1, x2, x3)
  x1 = x1.tolist()
  x2 = x2.tolist()
  x3 = x3.tolist()
  
  xx1 = []
  xx2 = []
  xx3 = []
  for i in range (0,len(x1)):
      for j in range (0,len(x2)):
        for k in range (0,len(x3)):
          xx1.append(x1[i][j][k])
          xx2.append(x2[i][j][k])
          xx3.append(x3[i][j][k])
  xx = pd.DataFrame(np.column_stack((xx1,xx2,xx3)))
  return xx

def mergeDict(dict1, dict2):
   ''' Merge dictionaries and keep values of common keys in list'''
   dict3 = {**dict1, **dict2}
   for key, value in dict3.items():
       if key in dict1 and key in dict2:
               dict3[key] = [value , dict1[key]]
   return dict3

def HeatMap3D(grip_model):
  ''' 3D plot coloured by a 4th variable'''
  #https://stackoverflow.com/questions/14995610/how-to-make-a-4d-plot-with-matplotlib-using-arbitrary-data
  fig = plt.figure(figsize=[10,10])
  ax = fig.add_subplot(111, projection='3d')

  x = grip_model.iloc[:,0]
  y = grip_model.iloc[:,1]
  z = grip_model.iloc[:,2]
  c = grip_model.iloc[:,-1]

  img = ax.scatter(x, y, z, c=c, alpha=1, cmap=plt.get_cmap('rainbow'))     #Colors: https://matplotlib.org/stable/gallery/color/colormap_reference.html
  cbar = fig.colorbar(img,shrink=0.5, aspect=20)                   #Size of the colorbar
  names = list(grip_model)
  
  ax.set_xlabel(names[0], fontsize=14)
  ax.set_ylabel(names[1], fontsize=14)
  ax.set_zlabel(names[2], fontsize=14, labelpad=18)
  ax.set_title('Heat Map 3D', fontsize=20)
  cbar.ax.get_yaxis().labelpad = 10
  cbar.ax.set_ylabel(names[-1], fontsize=14)
  
  return plt.show()

"""# Download data and check quality

### Import data
"""

dt = pd.read_excel('/content/drive/MyDrive/Quatrimestre 8/Dades i codi/A-BCD.xlsx', sheet_name="Model15")
dt = dt.rename(columns={'Unnamed: 0':'Sample'})               # Change the name of the column
dt.head()

## Tornar a comprovar la qualitat de les dades un cop eliminats els NaN:
#dt.info()                  # Indica si hi ha valors nuls i el tipus de dades en totes les columnes
#dt.boxplot()               # Permet veure el set de dades gràficament i si cal normalitzar-les.
#dt[dt.isna().any(axis=1)]  # Detecta els NaN
#dt.isnull().any()          # Detecta si hi ha NaN
dt.describe()              # Veu la mateixa informació que el boxplot però numèricament

"""## Check NaN's"""

NaN = dt[dt.isna().any(axis=1)]  # Detecta els NaN
NaN = NaN.iloc[: , 1:4]

x='NT'
y='RR'
plt.figure(figsize=[5,5])
plt.xlabel(x)
plt.ylabel(y)
plt.scatter(NaN.loc[:,x], NaN.loc[:,y], c='r', s=30)

#NaN position in dt
dt['class'] = dt.isna().any(axis=1)
dt.loc[dt['class']==True, 'class']='NaN'
dt.loc[dt['class']==False,'class']='OK'

getIndexes(dt,'NaN')[:]

names = ['OK', 'NaN']
dt_short = dt.iloc[:10,:]

x = 'Sample'
y = 'Distillate_Rate'

plt.figure(figsize=[5,5])
for n,c in zip(names, "gr"):                   #La tuppla rgb correspon als colors red, green blue. El zip permetrà identificar cada classe amb un color
  X = dt_short[dt_short['class']==n].loc[:,x]
  Y = dt_short[dt_short['class']==n].loc[:,y]              #Agafar la columna 0 (primer component)de la matriu Xp de totes les samples que tenen la classe n (comparem n amb la columna Xp['class'])
  plt.scatter(X, Y, c=c, label=n, s=50, alpha=1)  #Printar cadascuna de les classes amb el color corresponent i una opacitat alpha.
  plt.plot(dt_short.loc[:,x], dt_short.loc[:,y], c='k', alpha=0.2)  #Printar cadascuna de les classes amb el color corresponent i una opacitat alpha.
plt.xlabel(x)
plt.ylabel(y)
plt.legend()
plt.title("Evolution of NaN's in the simulation")
plt.show()

#Eliminar aquelles mostres que no han convergit (tenen NaN's):
dt = dt.dropna()
#dt[dt.isna().any(axis=1)]

"""## Costs"""

## Definir els costos (variables de sortida del model)
#Canvis d'unitats en les variables extretes del codi Python
dt['T_Condenser']    = dt['T_Condenser'] + 273.15             # ºC -> K
dt['T_Reboiler' ]    = dt['T_Reboiler' ] + 273.15             # ºC -> K
dt['Duty_Condenser'] = abs(dt['Duty_Condenser'])              # kJ/s

Sec_Year = 3600 * 24 * 300                         # sec/year
CEPCI_old = 470
CEPCI_act = 638.8

#Amortització
i = 0.2                                            # Interest rate
t = 5                                              # Expected life time of the project (years)
amort = i * (1+i)**t / ((1+i)**t - 1)              # Return of the investment (1/year)

#Costos fixes (tenen associat un installation factor)
C_Cond   = 3.5 * (24000 + 46 * dt['Area_Condenser']**1.2)
C_Reb    = 3.5 * (24000 + 46 * dt['Area_Reboiler']**1.2)
C_Trays  = 6   * (110 + 380 * dt['Column_Diameter']**1.8)  * dt['NT']

Shell_mass =  7900 * np.pi * (dt['NT']+2) * dt['Column_Spacing'] * ((dt['Column_Diameter']/2+0.007)**2-(dt['Column_Diameter']/2)**2) + 4/3*np.pi*((dt['Column_Diameter']/2+0.007)**3 - (dt['Column_Diameter']/2)**3)
C_Column = 4   * (15000 + 68 * Shell_mass**0.85)

#Costos variables (paper "HowToEstimateUtilityCosts")
C_CU     = (0.6  * dt['Duty_Condenser']**-0.9* dt['T_Condenser']**-3 * CEPCI_act + 1.1e6* dt['T_Condenser']**-5 * 4.5) * dt['Duty_Condenser']* Sec_Year 
C_HU     = (7e-7 * dt['Duty_Reboiler']**-0.9 * dt['T_Reboiler' ]**0.5* CEPCI_act + 6e-8 * dt['T_Reboiler']**0.5 * 4.5) * dt['Duty_Reboiler'] * Sec_Year

#Costos totals ($/year)
C_TOTAL = round((C_Cond + C_Reb + C_Trays + C_Column)*amort + C_HU + C_CU,0)

#Substitució de columnes
dt['HK_top'] = dt.loc[:,'TOP_Flow_B'] / dt.loc[:,'Distillate_Rate']
dt['LK_bottom'] = dt.loc[:,'BOTTOM_Flow_A'] / (dt.loc[:,'BOTTOM_Flow_A']+dt.loc[:,'BOTTOM_Flow_B']+dt.loc[:,'BOTTOM_Flow_C']+dt.loc[:,'BOTTOM_Flow_D'])
dt['TOP_Fraction_A'] = round(dt['TOP_Flow_A'] / dt['Distillate_Rate'],4)
dt['Recovery']       = dt['TOP_Flow_A'] / (dt['TOP_Flow_A']+dt['BOTTOM_Flow_A'])
dt.loc[dt['TOP_Fraction_A']>1,'TOP_Fraction_A'] = 1
dt.loc[dt['Recovery']>1,'Recovery'] = 1

dt['Cost/kgTOP']     = C_TOTAL / (dt['Distillate_Rate']* Sec_Year)
#dt['ObjF']           = dt['Cost/kgTOP'] * 1000 * (1-dt['Recovery'])

dt_useless = dt
dt = dt.loc[:,['NT','RR','Distillate_Rate','TOP_Fraction_A','Recovery','Cost/kgTOP']]

dt['Distillate_Rate'] = dt['Distillate_Rate']*3600   # kmol/s -> kmol/h
dt.iloc[:,0]  = round(dt.iloc[:,0],0)
dt.iloc[:,1:-1] = round(dt.iloc[:,1:-1],3)
dt.iloc[:,-1] = round(dt.iloc[:,-1],4)
dt.head()

Percentatge_Costs = pd.DataFrame()

Percentatge_Costs['%Cond']   = C_Cond/C_TOTAL*100*amort
Percentatge_Costs['%Reb' ]   = C_Reb/C_TOTAL*100*amort
Percentatge_Costs['%Trays']  = C_Trays/C_TOTAL*100*amort
Percentatge_Costs['%Column'] = C_Column/C_TOTAL*100*amort
Percentatge_Costs['%HU']     = C_HU/C_TOTAL*100
Percentatge_Costs['%CU']     = C_CU/C_TOTAL*100

Percentatge_Costs['%TOTAL']  = C_TOTAL/C_TOTAL*100

Percentatge_Costs.mean()

dt[dt.isna().any(axis=1)]

Cost_min_ini = pd.DataFrame(dt.iloc[getIndexes(dt,dt.iloc[:,-1].min())[0], :])
Cost_min_ini

"""## Heat Map"""

dt.head()

dt.to_csv('/content/drive/MyDrive/Quatrimestre 8/Dades i codi/Data/ini_HeatMap3D')

dt.describe()

# Heat map per veure com estan distribuits els punts, els costos.
Data_2D = dt[dt['NT']==8]
HeatMap(Data_2D,1,2,-1)

HeatMap3D(dt)

"""# Divide into a training and testing set. Normalize"""

Recovery = dt.loc[:,'Recovery']
TOP_Fraction_A= dt.loc[:,'TOP_Fraction_A']
dt = dt.drop(labels=['Recovery','TOP_Fraction_A'], axis=1)

## Barrejar les dades
dt = pd.DataFrame(shuffle(dt))
dt.head()

## Definir el percentatge de training set i validation set
#El threshold separa el data set prèviament barrejat per crear un training set i un data set
ths = dt.shape[0] * 9 // 10
ths

dt.describe()

## Normalitzar dades
nrm = StandardScaler()
nrm.fit(dt.iloc[:ths,:])
dt.iloc[:,:] = pd.DataFrame(nrm.transform(dt.iloc[:,:]), columns=dt.columns[:], index=dt.index)

dt.head()

dt.boxplot()

nrm_mean= nrm.mean_
nrm_std = nrm.var_**0.5 

nrm_mean, nrm_std

## Definir el training set i el validation set
input_variables = len(dt.iloc[0,:-1])

x_train = dt.iloc[:ths,:input_variables]
y_train = dt.iloc[:ths,input_variables:]

x_test = dt.iloc[ths:,:input_variables]
y_test = dt.iloc[ths:,input_variables:]

points = len(x_train)
print('Input variables: ', input_variables)

x_train.to_csv('/content/drive/MyDrive/Quatrimestre 8/Dades i codi/Data/x_train')
x_test.to_csv('/content/drive/MyDrive/Quatrimestre 8/Dades i codi/Data/x_test')
y_train.to_csv('/content/drive/MyDrive/Quatrimestre 8/Dades i codi/Data/y_train')
y_test.to_csv('/content/drive/MyDrive/Quatrimestre 8/Dades i codi/Data/y_test')

dt.describe()

"""# Create the ANN"""

model = Sequential([
      Dense(6, input_shape= (input_variables,), activation= 'relu'),  #Adding first layer (number of nodes, inputs(number of variables, activation function))
      Dense(3, activation= "relu"),                                   #Adding hidden layer 
      Dense(1 , activation= "linear")                                  #Adding output layer (number of nodes = number of outputs, activation)
])
model.summary()

model.compile(
    optimizer= 'adam',      #Es pot modificar el learning rate (lr=1e-4).
    loss= 'mean_squared_error',
    metrics=['mae','mse'])
t = TicToc() 
t.tic()
history = model.fit(
    x_train, y_train,
    batch_size= 32,
    epochs= 75,
    verbose= 0,
    validation_split= 0.15,    #Divideix el x_train en dues parts (training i prediction set)
    callbacks=[tfdocs.modeling.EpochDots(report_every=25)])
t.toc()
layers = model.layers[0]
w,b = layers.get_weights()

t.elapsed

"""# Predictions and accuracy

> **x_train, y_train:** punts reals per entrenar el model

> **x_test,  y_test :** punts reals per validar el model

> **x_model_trained, y_model_trained:** prediccions del model per al conjunt train

> **x_model_tested , y_model_tested :** prediccions del model per al conjunt test
"""

y_model_trained = model.predict(x_train)      #Prediccions dels punts d'entrenament
y_model_tested  = model.predict(x_test)       #Prediccions dels punts de testeig

residuals_train= y_model_trained - y_train    #Residus dels punts d'entrenament
residuals_test = y_model_tested  - y_test     #Residus dels punnts de testeig

LOSS = round(model.evaluate(x_test, y_test, verbose=0)[0],4)                              #Ens indica el underfitting. Es com el r2 però sense normalitzar
MSE = round(mean_squared_error(y_test.to_numpy().tolist(), y_model_tested.tolist()),4)    #Ens indica el overfitting

R2 = round(r2_score(y_model_tested.tolist(), y_test.to_numpy().tolist()),4)               #Ens indica el underfitting. Es com el evaluate
#MEAN = round(mean(y_model_tested.tolist(), y_test.to_numpy().tolist()),4)                 #Ens indica el underfitting. Es com el evaluate
#STD = round(std(y_model_tested.tolist(), y_test.to_numpy().tolist()),4)                   #Ens indica el underfitting. Es com el evaluate

print('MSE:', MSE)
print('R2:', R2)
print('Loss:', LOSS)
#print('Mean:', MEAN)
#print('Std:', STD)

"""#Plots

## 1.   Evolució de l'error en funció del nombre de epoch
"""

fig = plt.figure()
plt.plot(history.history['loss'], label="MSE")
plt.plot(np.sqrt(history.history['val_loss']), label='Validation_MSE')
plt.xlabel('epoch')
plt.ylabel('Mean Square Error')
plt.title('Loss vs. epoch')
plt.legend()

"""## 2.   Calcular i representar els residus, tant del training com del testing set

"""

# Desnormalitzem
y_model_trainedR = model.predict(x_train)*nrm_std[-1] + nrm_mean[-1]      #Prediccions dels punts d'entrenament
y_model_testedR  = model.predict(x_test)*nrm_std[-1] + nrm_mean[-1]       #Prediccions dels punts de testeig

x_trainR = x_train * nrm_std[:-1] + nrm_mean[:-1]
x_testR  = x_test  * nrm_std[:-1] + nrm_mean[:-1]
y_trainR = y_train * nrm_std[-1]  + nrm_mean[-1]
y_testR  = y_test  * nrm_std[-1]  + nrm_mean[-1]

residuals_trainR= y_model_trainedR - y_trainR    #Residus dels punts d'entrenament
residuals_testR = y_model_testedR  - y_testR     #Residus dels punnts de testeig

X = 2
fig = plt.figure()
plt.scatter(x_trainR.iloc[:,X], residuals_trainR.iloc[:,-1], s=50, marker='+', c='m', label='Training set')
plt.scatter(x_testR.iloc[:,X],  residuals_testR.iloc[:,-1],  s=100,marker='.', c='g', label='Testing set')
plt.plot([x_trainR.iloc[:,X].min(),x_trainR.iloc[:,X].max()], [0,0], 'grey')
plt.xlabel(list(x_trainR.columns)[X])
plt.ylabel('residuals')
plt.title('Residuals')
plt.legend(fontsize='10')
plt.show()

"""## Linear Regression"""

#train = y_trainR.values.reshape(-1,1)
#model_trained = y_model_trainedR.values.reshape(-1,1)
lin_reg = LinearRegression()
lin_reg.fit(y_testR,y_model_testedR)
regression = lin_reg.predict(y_testR)
R2_res = round(r2_score(y_model_testedR, regression),4)
R2_res

X = 1
fig = plt.figure(figsize=[6,6])
plt.scatter(y_testR, y_model_testedR,  s=20, alpha=0.8, marker='.', c='blue', label='Testing set')
plt.plot(y_testR, regression, color='red', label='Ideal value')
plt.xlabel('Cost', size=12)
plt.ylabel('Model cost prediction', size=12)
plt.title('Residuals')
plt.legend(fontsize='12')
plt.show()

"""## 3. Representar el model, i el training i testing set. Crear graella de punts"""

low = [dt.iloc[:,0].min(), dt.iloc[:,1].min(), dt.iloc[:,2].min()]
up = [dt.iloc[:,0].max(), dt.iloc[:,1].max(), dt.iloc[:,2].max()]

size = 15

x_model = grid_3D(low, up, size)
x_model.columns = [dt.keys()[0],dt.keys()[1],dt.keys()[2]]
y_model = pd.DataFrame(model.predict(x_model))
y_model.columns = [dt.keys()[-1]]

grip_model = pd.DataFrame(x_model.join(y_model) * nrm_std + nrm_mean)
grip_model.iloc[:,1:] = round(grip_model.iloc[:,1:],4)
grip_model['NT'] = round(grip_model['NT'],0)
grip_model.describe()

grip_model.to_csv('/content/drive/MyDrive/Quatrimestre 8/Dades i codi/Data/ANN_HeatMap3D')

# 2D Plot
Data_2D = grip_model[grip_model['NT']==8]

fig = plt.figure(figsize=(10,10))
ax = fig.add_subplot(111,projection='3d')
ax.scatter3D(Data_2D.iloc[:,1], Data_2D.iloc[:,2], Data_2D.iloc[:,-1], alpha=0.1)
ax.scatter(x_trainR.iloc[:,1], x_trainR.iloc[:,2], y_trainR.iloc[:,-1], alpha=0.5, s=30, c='m', label='Training set')
ax.scatter(x_testR.iloc[:,1],  x_testR.iloc[:,2],  y_testR.iloc[:,-1],  alpha=1, s=30, c='g', label='Validation set')
ax.set_xlabel('RR', labelpad=18, size = 14)
ax.set_ylabel('Distillate_Rate', labelpad=18, size = 14)
ax.set_zlabel('Cost/kgTOP', labelpad=18, size = 14)
ax.set_title('ANN model and samples', size='20')
ax.view_init(10, 120)     #Firt parameter changes the elevation, second parameter the rotation
plt.legend(fontsize=12)
plt.show

"""## 4. Heat Maps"""

# 2D PLOT
Data_2D = grip_model[grip_model['NT']==8]
HeatMap(Data_2D,1,2,-1)

HeatMap3D(grip_model)

"""# Export the model"""

"""
''' NORMALITZACIÓ'''

#S'ha d'exportar la normalització per a poder traduir les dades a predir per al model.
import pickle

with open('/content/drive/MyDrive/Quatrimestre 8/Dades i codi/A-BCD_ANN_normalize.pickle', 'wb') as f:
    pickle.dump(nrm, f, pickle.HIGHEST_PROTOCOL)
"""

"""
nrm = pickle.load('path_to_my_model.h5')
"""

''' MODEL '''

#Amb el pickle no es poden exportar models de Keras. La llibreria té el seu propi exportador.
#https://www.tensorflow.org/tutorials/keras/save_and_load
#https://www.tensorflow.org/guide/keras/save_and_serialize?hl=es-419

model.save('/content/drive/MyDrive/Quatrimestre 8/Dades i codi/Data/ANN_model_sim.keras')
print('Exported :)')
"""
from tensorflow import keras
model = keras.models.load_model('path_to_my_model.h5')
"""